{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2: Perform validation on hourly or daily data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import xarray as xr\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta, date\n",
    "\n",
    "import dask\n",
    "import dask.array as dda\n",
    "import dask.distributed as dd\n",
    "\n",
    "# rhodium-specific kubernetes cluster configuration\n",
    "import rhg_compute_tools.kubernetes as rhgk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client, cluster = rhgk.get_big_cluster()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test a few validation functions \n",
    "'''test_file = xr.open_dataset(os.path.join('/gcs/impactlab-data/climate/source_data/ERA-5/{}/{}/netcdf/F320/'.format('tas', 'hourly'), \n",
    "                                         't2m_1998_12_20.nc'))'''\n",
    "test_file = xr.open_dataset(os.path.join('/gcs/impactlab-data/climate/source_data/ERA-5/{}/{}/netcdf/F320/'.format('pr', 'hourly'), \n",
    "                                         'total_precip_1998_12_20.nc'))\n",
    "# test = test_file['t2m'][:6, :2, :2]\n",
    "# ds = test_file['tp'][:6, :2, :2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions for validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation functions\n",
    "def test_for_nans(ds, var):\n",
    "    \"\"\"\n",
    "    test for presence of NaNs\n",
    "    \"\"\"\n",
    "    assert ds[var].isnull().sum() == 0, \"there are nans!\"\n",
    "\n",
    "def test_timesteps(ds, timestep):\n",
    "    \"\"\"\n",
    "    correct number of timesteps \n",
    "    \"\"\"\n",
    "    if timestep == 'hourly':\n",
    "        assert (len(ds.time) == 24), \"there are not 24 hours in this file!\"\n",
    "    elif timestep == 'daily':\n",
    "        assert (len(ds.time) == 365 or len(ds.time) == 366), \"there are not 365 or 366 days in this file!\"\n",
    "    \n",
    "def test_temp_range(ds, var):\n",
    "    \"\"\"\n",
    "    make sure temp values are in a valid range. valid for tas, tasmin, tasmax \n",
    "    \"\"\"\n",
    "    assert (ds[var].min() > 150) and (ds[var].max() < 350), \"{} values are invalid\".format(var) \n",
    "    \n",
    "def test_dtr_range(ds, var):\n",
    "    \"\"\"\n",
    "    make sure DTR values are in a valid range\n",
    "    \"\"\"\n",
    "    assert (ds[var].min() > 0) and (ds[var].max() < 40), \"diurnal temperature range values are invalid\" \n",
    "\n",
    "def test_low_temp_range(ds, var): \n",
    "    \"\"\"\n",
    "    if we have some really low temp values, we want to know. valid for tas, tasmin, tasmax  \n",
    "    \"\"\"\n",
    "    threshold = 180 # K\n",
    "    return ds[var].where(ds[var] < threshold).count()\n",
    "\n",
    "def test_high_temp_range(ds, var):\n",
    "    \"\"\"\n",
    "    if we have some really high temp values, we want to know. valid for tas, tasmin, tasmax  \n",
    "    \"\"\"\n",
    "    threshold = 330 # K\n",
    "    return ds[var].where(ds[var] > threshold).count()\n",
    "\n",
    "def test_polar_high_temp(ds, var):\n",
    "    \"\"\"\n",
    "    if we have some really low or high polar temp values, we want to know. valid for tas, tasmin, tasmax  \n",
    "    \"\"\"\n",
    "    threshold = 317 #315.5 # K\n",
    "    loc_NH = ds[var].sel(latitude=slice(90,50), drop=True)\n",
    "    num_NH = loc_NH.where(loc_NH > threshold).count()\n",
    "    \n",
    "    loc_SH = ds[var].sel(latitude=slice(-50,-90), drop=True)\n",
    "    num_SH = loc_SH.where(loc_SH > threshold).count()\n",
    "    return num_NH, num_SH\n",
    "\n",
    "def test_negative_values(ds, var):\n",
    "    \"\"\"\n",
    "    test for presence of negative values. valid for DTR or precip \n",
    "    \"\"\"\n",
    "    # this is not set to 0 to deal with floating point error \n",
    "    assert ds[var].where(ds[var] < -0.001).count() == 0, \"there are negative values!\"\n",
    "\n",
    "def test_maximum_precip(ds, var):\n",
    "    \"\"\"\n",
    "    test that max precip is reasonable \n",
    "    \"\"\"\n",
    "    threshold = 2.0 # max observed is 1.825m --> maximum occurs between 0.5-0.8\n",
    "    return ds[var].where(ds[var] > threshold).count()\n",
    "\n",
    "def validate_era5_variable(spec): \n",
    "    \"\"\"\n",
    "    validate ERA-5 hourly or daily files. \n",
    "    valid for hourly `t2m` or `pr` OR daily tasmin, tasmax, DTR, pr\n",
    "    \"\"\"\n",
    "\n",
    "    filepath, timestep, var = spec\n",
    "    print(spec)\n",
    "    \n",
    "    # first check to be sure file exists\n",
    "    if os.path.isfile(filepath):\n",
    "        pass\n",
    "    else:\n",
    "        raise FileNotFoundError(\"%s was not created\" %filepath)\n",
    "    \n",
    "    # now validate \n",
    "    temperature_vars = {\"t2m\", \"tasmin\", \"tasmax\"}\n",
    "    \n",
    "    try: \n",
    "    \n",
    "        with xr.open_dataset(filepath) as ds:\n",
    "\n",
    "            # validation checks for all variables \n",
    "            try: \n",
    "                test_for_nans(ds, var)\n",
    "            except AssertionError: \n",
    "                return (\"NaNs!\", filepath) \n",
    "            test_timesteps(ds, timestep)\n",
    "\n",
    "            if var in temperature_vars: \n",
    "                # temperature specific checks \n",
    "                test_temp_range(ds, var)\n",
    "                occurrances_low = test_low_temp_range(ds, var)\n",
    "                if occurrances_low > 0:\n",
    "                    return [occurrances_low, filepath]\n",
    "                occurrances_high = test_high_temp_range(ds, var)\n",
    "                if occurrances_high > 0:\n",
    "                    return [occurrances_high, filepath]\n",
    "                [occur_NH, occur_SH] = test_polar_high_temp(ds, var)\n",
    "                if occur_NH or occur_SH > 0:\n",
    "                    return [occur_NH, occur_SH, filepath]\n",
    "            elif var == \"tp\" or var == 'pr':\n",
    "                # precip specific checks \n",
    "                try: \n",
    "                    test_negative_values(ds, var)\n",
    "                except AssertionError: \n",
    "                    return (\"negative values\", filepath)\n",
    "\n",
    "                max_occurrances = test_maximum_precip(ds, var)\n",
    "                if max_occurrances > 0:\n",
    "                    return [max_occurrances, filepath]\n",
    "            elif var == \"dtr\":\n",
    "                # DTR specific checks\n",
    "                try: \n",
    "                    test_dtr_range(ds, var)\n",
    "                except AssertionError: \n",
    "                    return (\"invalid DTR range\", filepath)\n",
    "                test_negative_values(ds, var)\n",
    "            else: \n",
    "                raise ValueError(\"this variable is not supported in the current validation routines\")\n",
    "    except OSError:\n",
    "        return (\"unknown file format\", filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_JOBS_files(variable, start_date, end_date, timestep):\n",
    "    \n",
    "    if variable == 't2m' and timestep == 'hourly':\n",
    "        direc_var = 'tas'\n",
    "        filename_var = 't2m'\n",
    "        file_var = 't2m'\n",
    "    elif variable == 'pr' and timestep == 'hourly':\n",
    "        direc_var = 'pr'\n",
    "        filename_var = 'total_precip'\n",
    "        file_var = 'tp'\n",
    "    else:\n",
    "        direc_var = variable\n",
    "        filename_var = variable\n",
    "        file_var = variable\n",
    "        \n",
    "    # make list of daily datetime indices, this includes leap years \n",
    "    dt_index_full = pd.date_range(start=start_date, end=end_date, freq='D')\n",
    "    \n",
    "    # reformat month/day for the retrieval function \n",
    "    dt_index_years = dt_index_full.year.astype(str)\n",
    "    dt_index_months = dt_index_full.month.map(\"{:02}\".format)\n",
    "    dt_index_days = dt_index_full.day.map(\"{:02}\".format)\n",
    "    \n",
    "    if timestep == 'hourly':\n",
    "        directory = '/gcs/impactlab-data/climate/source_data/ERA-5/{}/{}/netcdf/F320/'.format(direc_var, timestep)\n",
    "        daily_files = ['%s_%s_%s_%s.nc' %(filename_var, year, month, day) for year, month, \n",
    "               day in zip(dt_index_years, dt_index_months, dt_index_days)]\n",
    "    elif timestep == 'daily':\n",
    "        directory = '/gcs/impactlab-data/climate/source_data/ERA-5/{}/daily/netcdf/v1.2'.format(variable)\n",
    "        daily_files = ['%s_daily_%s-%s.nc' %(filename_var, year, year) for year in np.unique(dt_index_years)]\n",
    "    \n",
    "    daily_filepaths = [os.path.join(directory, daily_file) for daily_file in daily_files]\n",
    "    JOBS_validation = [(filepath, timestep, file_var) for filepath in daily_filepaths]\n",
    "    return JOBS_validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting validation ##\n",
    "\n",
    "This validation script works for the following hourly variables: \n",
    "\n",
    "hourly_vars = `t2m`, `pr`\n",
    "\n",
    "And daily variables:\n",
    "\n",
    "daily_vars = `tasmin`, `tasmax`, `dtr`, `pr`\n",
    "\n",
    "NOTE: for doing daily versus hourly validation, be sure to update the `timestep` setting below. Also update `era_start` and `era_end` depending on what range of hourly or daily files you want to validate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time period for validation \n",
    "'''era_start = '12-17-1995'\n",
    "era_end = '01-15-2015'\n",
    "'''\n",
    "era_start = '01-01-1995'\n",
    "era_end = '12-31-2000'\n",
    "var = 'tasmin'\n",
    "timestep = 'daily'\n",
    "\n",
    "JOBS = generate_JOBS_files(var, era_start, era_end, timestep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del futures_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "futures_validation = client.map(validate_era5_variable, JOBS)\n",
    "dd.progress(futures_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gathers output from workers\n",
    "results = client.gather(futures_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test number of not None instances in results i.e. where a function 'failed'\n",
    "if results is not None:\n",
    "    print(sum(x is not None for x in results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Only execute cell below is above test NOT 0. Not needed otherwise.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saves the output of above -- identifying where a value (and not None) is located\n",
    "if results is not None:\n",
    "    list_results = [x is not None for x in results]\n",
    "\n",
    "    # worker index for flagged output (if above = True)\n",
    "    res = [i for i, val in enumerate(list_results) if val]\n",
    "\n",
    "    for i in res:\n",
    "        print(results[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "2072bfdee441421399c4474983d735ca": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "VBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_764c69a3ebb14ee3ae2f0fc149c016ab"
       ],
       "layout": "IPY_MODEL_ac08f6a6558a4237b8a6013e6ec1fa19"
      }
     },
     "33e58bd0e8204ce6863675f471e588d7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "68fd4c27747b4f92b347fdb55f38ecbf": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_8aff2a7b1edd48328edee97736ac11c2",
       "style": "IPY_MODEL_822152806945411ea16238204104e4c8",
       "value": "<div style=\"padding: 0px 10px 0px 10px; text-align: right\">15341 / 15341</div>"
      }
     },
     "707045b80ec342a2b652fe49b58cda11": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "764c69a3ebb14ee3ae2f0fc149c016ab": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_68fd4c27747b4f92b347fdb55f38ecbf",
        "IPY_MODEL_9269a20117924b8680188f89181937c8",
        "IPY_MODEL_936c21762e3e4c56ad74ea811f50bd25"
       ],
       "layout": "IPY_MODEL_94415c24a4354da2808ae222c50575e3"
      }
     },
     "822152806945411ea16238204104e4c8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "87e47c61e1a142c18cadb2fac0f7d113": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "8aff2a7b1edd48328edee97736ac11c2": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "9269a20117924b8680188f89181937c8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_a1ffe84641204a2dafd8752027d2f9f9",
       "max": 1,
       "style": "IPY_MODEL_ca7fada0755349bc9c107d3696619246",
       "value": 1
      }
     },
     "936c21762e3e4c56ad74ea811f50bd25": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_87e47c61e1a142c18cadb2fac0f7d113",
       "style": "IPY_MODEL_707045b80ec342a2b652fe49b58cda11",
       "value": "<div style=\"padding: 0px 10px 0px 10px; text-align:left; word-wrap: break-word;\">calc_daily_dinural_temp_range</div>"
      }
     },
     "94415c24a4354da2808ae222c50575e3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "a1ffe84641204a2dafd8752027d2f9f9": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "ac08f6a6558a4237b8a6013e6ec1fa19": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "b03845cc3e654a35ba6c8665a8bff699": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "b9c11d31fe034f5d865db04aaad20b3c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "VBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_d4b92763113a46ea86ceb2cee4c4610f",
        "IPY_MODEL_2072bfdee441421399c4474983d735ca"
       ],
       "layout": "IPY_MODEL_b03845cc3e654a35ba6c8665a8bff699"
      }
     },
     "ca7fada0755349bc9c107d3696619246": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "d4b92763113a46ea86ceb2cee4c4610f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_feb1139d30014a1a815f6bf051e24e0b",
       "style": "IPY_MODEL_33e58bd0e8204ce6863675f471e588d7",
       "value": "<div style=\"padding: 0px 10px 5px 10px\"><b>Finished:</b>  9min 48.5s</div>"
      }
     },
     "feb1139d30014a1a815f6bf051e24e0b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
