{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3: Create year long files of daily tas, tasmax, tasmin, dtr, and total_precip.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import xarray as xr\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta, date\n",
    "\n",
    "import dask\n",
    "import dask.array as dda\n",
    "import dask.distributed as dd\n",
    "\n",
    "# rhodium-specific kubernetes cluster configuration\n",
    "import rhg_compute_tools.kubernetes as rhgk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "client, cluster = rhgk.get_standard_cluster()\n",
    "cluster.scale(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>gateway://traefik-impactlab-hub-dask-gateway.impactlab-hub:80/impactlab-hub.dcbf119d795d457da558e2655cff0e86</li>\n",
       "  <li><b>Dashboard: </b><a href='/services/dask-gateway/clusters/impactlab-hub.dcbf119d795d457da558e2655cff0e86/status' target='_blank'>/services/dask-gateway/clusters/impactlab-hub.dcbf119d795d457da558e2655cff0e86/status</a></li>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>30</li>\n",
       "  <li><b>Cores: </b>30</li>\n",
       "  <li><b>Memory: </b>362.39 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'tls://10.20.22.3:8786' processes=30 threads=30, memory=362.39 GB>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions to create yearlong files of daily data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_daily_era5_average(spec):\n",
    "    '''\n",
    "    calculate daily-averaged ERA-5 temperature data \n",
    "    '''\n",
    "    filepath, timestep, variable = spec\n",
    "    var = variable\n",
    "    with xr.open_dataset(filepath) as ds:\n",
    "        return(ds[var].mean('time'))\n",
    "\n",
    "def calc_daily_era5_tmax(spec):\n",
    "    '''\n",
    "    calculate daily-averaged ERA-5 temperature data \n",
    "    '''\n",
    "    filepath, timestep, variable = spec\n",
    "    var = variable\n",
    "    with xr.open_dataset(filepath) as ds:\n",
    "        return(ds[var].max('time'))\n",
    "\n",
    "def calc_daily_era5_tmin(spec):\n",
    "    '''\n",
    "    calculate daily-averaged ERA-5 temperature data \n",
    "    '''\n",
    "    filepath, timestep, variable = spec\n",
    "    var = variable\n",
    "    with xr.open_dataset(filepath) as ds:\n",
    "        return(ds[var].min('time'))\n",
    "    \n",
    "def calc_daily_dinural_temp_range(spec):\n",
    "    '''\n",
    "    calculate daily-averaged diurnal temperature renage (DTR)  \n",
    "    '''\n",
    "    filepath, timestep, variable = spec\n",
    "    var = variable\n",
    "    with xr.open_dataset(filepath) as ds:\n",
    "        return(ds[var].max('time') - ds[var].min('time'))\n",
    "    \n",
    "def calc_daily_total_precip(spec):\n",
    "    '''\n",
    "    calculate daily sum of precipitation  \n",
    "    '''\n",
    "    filepaths, timestep, variable = spec\n",
    "    var = variable\n",
    "    with xr.open_mfdataset(filepaths, concat_dim='time', combine='by_coords') as ds:\n",
    "        data_rolled = ds[var].roll(time=-1, roll_coords=True)\n",
    "        da_resampled = data_rolled[:-1].resample(time='1D', closed='right').sum(dim='time')\n",
    "        return da_resampled[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_jobs(year, variable):\n",
    "    if variable == 'pr':\n",
    "        start = '01-01-{}'.format(year)\n",
    "        end = '01-01-{}'.format(year+1)\n",
    "        \n",
    "        director_var = 'pr' #precip\n",
    "        file_var = 'tp'\n",
    "        title_var = 'total_precip'\n",
    "        \n",
    "        # make list of daily datetime indices, this includes leap years \n",
    "        dt_index_full = pd.date_range(start=start, end=end, freq='D')\n",
    "        \n",
    "        # reformat month/day for the retrieval function \n",
    "        dt_index_years = dt_index_full.year.astype(str)\n",
    "        dt_index_months = dt_index_full.month.map(\"{:02}\".format)\n",
    "        dt_index_days = dt_index_full.day.map(\"{:02}\".format)\n",
    "        \n",
    "        daily_files = ['%s_%s_%s_%s.nc' %(title_var, year, month, day) for year, month, \n",
    "                   day in zip(dt_index_years, dt_index_months, dt_index_days)]\n",
    "        hourly_dir = '/gcs/impactlab-data/climate/source_data/ERA-5/{}/hourly/netcdf/F320/'.format(director_var)\n",
    "\n",
    "        daily_filepaths = [(os.path.join(hourly_dir, daily_files[i]), os.path.join(hourly_dir, daily_files[i+1])) for i in np.arange(0, len(daily_files[:-1]))]\n",
    "        JOBS = [(filepath, 'hourly', file_var) for filepath in daily_filepaths] \n",
    "        return [JOBS, dt_index_full]\n",
    "        \n",
    "    else:\n",
    "        director_var = 'tas'\n",
    "        file_var = 't2m'\n",
    "        \n",
    "        start = '01-01-{}'.format(year)\n",
    "        end = '12-31-{}'.format(year)\n",
    "\n",
    "        # make list of daily datetime indices, this includes leap years \n",
    "        dt_index_full = pd.date_range(start=start, end=end, freq='D')\n",
    "\n",
    "        # reformat month/day for the retrieval function \n",
    "        dt_index_years = dt_index_full.year.astype(str)\n",
    "        dt_index_months = dt_index_full.month.map(\"{:02}\".format)\n",
    "        dt_index_days = dt_index_full.day.map(\"{:02}\".format)\n",
    "\n",
    "        daily_files = ['%s_%s_%s_%s.nc' %(file_var, year, month, day) for year, month, \n",
    "                       day in zip(dt_index_years, dt_index_months, dt_index_days)]\n",
    "        hourly_dir = '/gcs/impactlab-data/climate/source_data/ERA-5/{}/hourly/netcdf/F320/'.format(director_var)\n",
    "\n",
    "        daily_filepaths = [os.path.join(hourly_dir, daily_file) for daily_file in daily_files]\n",
    "        JOBS = [(filepath, 'hourly', file_var) for filepath in daily_filepaths] \n",
    "        return [JOBS, dt_index_full]\n",
    "\n",
    "def save_yearlong_dailydata_file(directory, year, ds, var):\n",
    "    '''\n",
    "    save file of daily data for one variable for one year\n",
    "    directory(str)\n",
    "    year(str)\n",
    "    ds(Dataset)\n",
    "    var(str)\n",
    "    '''\n",
    "    today = str(date.today())\n",
    "    daily_file = xr.Dataset( {var: ds},\n",
    "                           attrs={\n",
    "        'author': 'Meredith Fish',\n",
    "        'contact': 'meredith.fish@rutgers.edu',\n",
    "        'project': ('global downscaling [ERA-5]'),\n",
    "        'source': ('impactlab-data/climate/source-data/ERA-5/{}/hourly/netcdf/'.format(var)),\n",
    "        'created': today})\n",
    "    #daily_file = daily_file.compute()\n",
    "    filename = '%s_daily_%s-%s.nc' %(var, year, year)\n",
    "    daily_file.to_netcdf(os.path.join(directory, filename), mode='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_daily_file_creation(year, variable):\n",
    "    '''\n",
    "    uses `create_jobs` to create the JOBS files. \n",
    "    calculate the metric based on variable of interest. \n",
    "    save netcdf file.\n",
    "    '''\n",
    "    \n",
    "    [JOBS, dt_index_full] = create_jobs(year, variable)\n",
    "\n",
    "    save_directory = '/gcs/impactlab-data/climate/source_data/ERA-5/{}/daily/netcdf/v1.2'.format(variable)\n",
    "\n",
    "    if variable == 'dtr':\n",
    "        calc = calc_daily_dinural_temp_range\n",
    "    elif variable == 'tas':\n",
    "        calc = calc_daily_era5_average\n",
    "    elif variable == 'tasmax':\n",
    "        calc = calc_daily_era5_tmax\n",
    "    elif variable == 'tasmin':\n",
    "        calc = calc_daily_era5_tmin\n",
    "    elif variable == 'pr':\n",
    "        calc = calc_daily_total_precip\n",
    "    \n",
    "    futures = client.submit(calc, JOBS)\n",
    "    da_list = client.gather(futures)\n",
    "\n",
    "    # concatenate DataArrays in list \n",
    "    da_year = xr.concat(da_list, dim='time')\n",
    "    # add datetime index \n",
    "    if variable == 'pr':\n",
    "        da_year['time'] = dt_index_full[:-1]\n",
    "    else:\n",
    "        da_year['time'] = dt_index_full\n",
    "    \n",
    "    # save file\n",
    "    save_yearlong_dailydata_file(save_directory, year, da_year, variable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Execute file creation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variable options: pr, dtr, tas, tasmax, tasmin (they correspond with the directory name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "var = 'tas'\n",
    "\n",
    "for i_yr in np.arange(1994,2016):\n",
    "    execute_daily_file_creation(i_yr, var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precip v1.3 created with function versions below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_daily_file_creation_precip(year, variable):\n",
    "    '''\n",
    "    uses `create_jobs` to create the JOBS files. \n",
    "    calculate the metric based on variable of interest. \n",
    "    save netcdf file.\n",
    "    '''\n",
    "    \n",
    "    [JOBS, dt_index_full] = create_jobs(year, variable)\n",
    "\n",
    "    save_directory = '/gcs/impactlab-data/climate/source_data/ERA-5/{}/daily/netcdf/v1.3'.format(variable)\n",
    "    \n",
    "    da_list = []\n",
    "    for iJOBS in JOBS:\n",
    "        da = calc_daily_total_precip(iJOBS)\n",
    "        da_list.append(da)\n",
    "\n",
    "    # concatenate DataArrays in list \n",
    "    da_year = xr.concat(da_list, dim='time')\n",
    "    # add datetime index \n",
    "    if variable == 'pr':\n",
    "        da_year['time'] = dt_index_full[:-1]\n",
    "    else:\n",
    "        da_year['time'] = dt_index_full\n",
    "    \n",
    "    da_year = da_year.chunk({'time':len(da_year.time), 'latitude': len(da_year['latitude']), 'longitude': len(da_year['longitude'])})\n",
    "    ds_year = da_year.to_dataset()\n",
    "    \n",
    "    return ds_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_yearlong_dailydata_file_precip(directory, year, ds, var):\n",
    "    '''\n",
    "    save file of daily data for one variable for one year\n",
    "    directory(str)\n",
    "    year(str)\n",
    "    ds(Dataset)\n",
    "    var(str)\n",
    "    '''\n",
    "    today = str(date.today())\n",
    "    filename = '%s_daily_%s-%s.nc' %(var, year, year)\n",
    "    \n",
    "    attrsdt = {\n",
    "                'author': 'Meredith Fish',\n",
    "                'contact': 'meredith.fish@rutgers.edu',\n",
    "                'project': ('global downscaling [ERA-5]'),\n",
    "                'source': ('impactlab-data/climate/source-data/ERA-5/{}/hourly/netcdf/'.format(var)),\n",
    "                'created': today}\n",
    "\n",
    "    ds.attrs.update(attrsdt)\n",
    "\n",
    "    ds.to_netcdf(os.path.join(directory, filename), mode='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "var = 'pr'\n",
    "directory = '/gcs/impactlab-data/climate/source_data/ERA-5/pr/daily/netcdf/v1.3'\n",
    "\n",
    "for i_yr in np.arange(1994,2016):\n",
    "    ds = execute_daily_file_creation_precip(i_yr, var)\n",
    "    save_yearlong_dailydata_file_precip(directory, i_yr, ds, var) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
